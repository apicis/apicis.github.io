<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <title>Tommaso Apicella's Personal Website</title>
    <link rel="stylesheet" href="./css/style.css">
    <link rel="stylesheet" href="./css/academicons.css">
    <link rel="stylesheet" href="./css/academicons.min.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
</head>

<body>
<header>
    <div class="contacts">
        <a href="https://linkedin.com/in/tommaso-apicella" class="icon" target="_blank" rel="noopener noreferrer"><i class="fa fa-linkedin"
                                                                           style="font-size:36px"></i></a>
        <a href="https://github.com/apicis" class="icon" target="_blank" rel="noopener noreferrer"><i class="fa fa-github" style="font-size:36px;"></i></a>
        <a href="https://scholar.google.com/citations?user=ITgSL4sAAAAJ&hl=it&oi=ao" class="icon" target="_blank" rel="noopener noreferrer"><i
                class="ai ai-google-scholar-square ai-3x" style="font-size:36px;"></i></a>
    </div>
</header>
<main>
    <section id="home">
        <h1> About me </h1>
        <div class="about">
            I am a 3rd-year PhD student in a collaboration between University of Genoa and Queen Mary University of
            London.
            <br>
            <br>
            The main topic of my research activity is Affordance Segmentation that identifies the surfaces of potential interaction between
            an
            agent (e.g. a robotic hand) and an object relying only on visual information. The exciting and fascinating
            aspect of Affordance Segmentation is the connection to robotic and prosthetic applications, enabling
            assistive technologies (e.g., grasping, object manipulation) or collaborative human-robot scenarios.
            <br>
            <br>
            During the PhD I designed and deployed deep learning models on resource-constrained devices such as
            NVIDIA Jetson boards, smartphones and Raspberry PI. I collaborated with other PhD students, took part to
            side projects related to Visual Sentiment Analysis, and co-supervised 3 Bachelor students theses.
        </div>
        <br>
        <br>
        <h1> Research interests </h1>
        <div class="interests">
            Computer Vision | Machine Learning | Affordance detection and segmentation | Resource-constrained devices
        </div>
        <br>
        <br>
    </section>
    <section id="publications">
        <h1> Selected Publications </h1>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
            <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="one">
                    <img src='' width="320">
                </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
                <strong>Segmenting Object Affordances: Reproducibility and Sensitivity to Scale</strong>
                <br>
                T. Apicella, A. Xompero, P. Gastaldo, A. Cavallaro
                <br>
                <em>European Conference on Computer Vision Workshops (ECCVW)</em>, 2024
                <br>
                [<a href="https://arxiv.org/abs/2409.01814">arXiv</a>]
                [<a href="https://apicis.github.io/aff-seg">website</a>]
                [<a href="https://github.com/apicis/aff-seg">code</a>]
                [models]
            </td>
            </tr>
            <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="one">
                    <img src='images/acanet.png' width="320">
                </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
                <strong>Affordance segmentation of hand-occluded containers from exocentric images</strong>
                <br>
                T. Apicella, A. Xompero, E. Ragusa, R. Berta, A. Cavallaro, P. Gastaldo
                <br>
                <em>International Conference on Computer Vision Workshops (ICCVW)</em>, 2023
                <br>
                [<a href="https://arxiv.org/abs/2308.11233">arXiv</a>]
                [<a href="./projects/acanet.html">website</a>]
                [<a href="https://github.com/SEAlab-unige/acanet">code</a>]
                [<a href="https://doi.org/10.5281/zenodo.8364196">model</a>]
                [<a href="https://doi.org/10.5281/zenodo.5085800">mixed-reality data</a>]
                [<a href="https://doi.org/10.5281/zenodo.10708553">real testing data</a>]
            </td>
            </tr>
            <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                    <div class="one">
                        <img src='images/icassp_2022.png' width="320">
                    </div>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                    <strong>Container Localisation and Mass Estimation with an RGB-D Camera</strong>
                    <br>
                    T. Apicella, G. Slavic, E. Ragusa, P. Gastaldo, L. Marcenaro
                    <br>
                    <em>International Conference on Acoustics, Speech, & Signal Processing (ICASSP)</em>, 2022
                    <br>
                    [<a href="https://ieeexplore.ieee.org/abstract/document/9747134" target="_blank" rel="noopener noreferrer">paper</a>]
                    [<a href="https://arxiv.org/abs/2203.01207" target="_blank" rel="noopener noreferrer">arXiv</a>]
                    [<a href="https://github.com/CORSMAL/Visual" target="_blank" rel="noopener noreferrer">code</a>]
                </td>
            </tr>
            <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                    <div class="one">
                        <img src='images/icecs_2021.png' width="320">
                    </div>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                    <strong>An Affordance Detection Pipeline for Resource-Constrained Devices</strong>
                    <br>
                    T. Apicella, A. Cavallaro, R. Berta, P. Gastaldo, F. Bellotti, E. Ragusa
                    <br>
                    <em>International Conference on Electronics Circuits and Systems (ICECS)</em>, 2021
                    <br>
                    [<a href="https://ieeexplore.ieee.org/document/9665447" target="_blank" rel="noopener noreferrer">paper</a>]
                    [<a href="https://github.com/SEAlab-unige/ICECS-2021" target="_blank" rel="noopener noreferrer">code</a>]
                </td>
            </tr>
            <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                    <div class="one">
                        <img src='images/cogn_comp_2021.jpg' width="320">
                    </div>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                    <strong>Design and Deployment of an Image Polarity Detector with Visual Attention</strong>
                    <br>
                    E. Ragusa, T. Apicella, C. Gianoglio, R. Zunino, P. Gastaldo
                    <br>
                    <em>Cognitive Computation</em>, 2021
                    <br>
                    [<a href="https://link.springer.com/article/10.1007/s12559-021-09829-6" target="_blank" rel="noopener noreferrer">paper</a>]
                    [<a href="https://github.com/SEAlab-unige/cogn-comp-2021" target="_blank" rel="noopener noreferrer">code</a>]
                </td>
            </tr>
            <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                    <div class="one">
                        <img src='images/sensors_2021.png' width="320">
                    </div>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                    <strong>Emotion Recognition on Edge Devices: Training and Deployment</strong>
                    <br>
                    V. Pandelea, E. Ragusa, T. Apicella, P. Gastaldo, E. Cambria
                    <br>
                    <em>MDPI Sensors</em>, 2021
                    <br>
                    [<a href="https://www.mdpi.com/1424-8220/21/13/4496" target="_blank" rel="noopener noreferrer">paper</a>]
                    [<a href="https://github.com/SEAlab-unige/Sensors-2021-Emotion" target="_blank" rel="noopener noreferrer">code</a>]
                </td>
            </tr>
            <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                    <div class="one">
                        <img width="320">
                    </div>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                    <strong>Analyzing Machine Learning on Mainstream Microcontrollers</strong>
                    <br>
                    V. Falbo, T. Apicella, D. Aurioso, L. Danese, F. Bellotti, R. Berta, A. De Gloria
                    <br>
                    <em>ApplePies</em>, 2019
                    <br>
                    [<a href="https://link.springer.com/chapter/10.1007/978-3-030-37277-4_12" target="_blank" rel="noopener noreferrer">paper</a>]
                </td>
            </tr>
            </tbody>
        </table>


    </section>
</main>
</body>
</html>
