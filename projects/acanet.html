<!DOCTYPE html>
<html lang="en">
<head>
    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-T0PEYZJE5E"></script>
    <script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-T0PEYZJE5E');

    </script>
    <meta charset="UTF-8">
    <title>Affordance segmentation of hand-occluded containers from exocentric images</title>
    <link rel="stylesheet" href="../css/style-acanet.css">
    <link rel="stylesheet" href="../css/academicons.css">
    <link rel="stylesheet" href="../css/academicons.min.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
</head>

<body>
<div>
    <h1>Affordance segmentation of hand-occluded containers from exocentric images</strong></h1>
    <!--<h3><em>International Conference on Computer Vision Workshops, 2023</em></h3>-->
    <h3>T. Apicella<sup>1,2</sup>, A. Xompero<sup>2</sup>, E. Ragusa<sup>1</sup>, R. Berta<sup>1</sup>,
        A. Cavallaro<sup>2,3,4</sup>, P. Gastaldo<sup>1</sup> <br/>
        <sup>1</sup>UniGe, <sup>2</sup>QMUL, <sup>3</sup>Idiap, <sup>4</sup>EPFL</h3>
    <br>
</div>
<div class="links">
    <a href="https://openaccess.thecvf.com/content/ICCV2023W/ACVR/papers/Apicella_Affordance_Segmentation_of_Hand-Occluded_Containers_from_Exocentric_Images_ICCVW_2023_paper.pdf" class="icon_publication"><i class="fa fa-file-pdf-o" style="font-size:20px;"> Paper (coming soon)</i></a>
    <a href="https://arxiv.org/abs/2308.11233" class="icon_publication"><i class="ai ai-arxiv ai-3x"
                                                                           style="font-size:20px;"> arXiv</i></a>
    <a href="https://github.com/SEAlab-unige/acanet" class="icon_publication"><i class="fa fa-github"
                                                                                 style="font-size:20px;"> Code</i></a>
    <a href="https://doi.org/10.5281/zenodo.8364196" class="icon_publication"><i class="ai ai-zenodo"
                                                                                 style="font-size:20px;"> Model</i></a>
    <!--<a href="https://doi.org/10.5281/zenodo.5085800" class="icon_publication"><i class="ai ai-zenodo" style="font-size:20px;"> Dataset</i></a>-->
    <br>
    <br>
    <br>
</div>

<!--<h2>Abstract</h2>-->
<figure>
    <img src="../images/acanet.png" alt="ACANet model">
    <!--<figcaption>ACANet block diagram </br>
        --<table>
            <tr>
                <th>Affordance branch output legend:&nbsp;&nbsp;</th>
                <th class="black">&nbsp;&nbsp;</th>
                <th><em>background,&nbsp;&nbsp;</em></th>
                <th class="red">&nbsp;&nbsp;</th>
                <th><em>graspable,&nbsp;&nbsp;</em></th>
                <th class="green">&nbsp;&nbsp;</th>
                <th><em>contain,&nbsp;&nbsp;</em></th>
                <th class="white">&nbsp;&nbsp;</th>
                <th><em>arm</em></th>
            </tr>
        </table> -->
    </figcaption>
</figure>

<div class="text">
    <p>Visual affordance segmentation identifies the surfaces of potential interaction with an object in an image. The
        variety of the geometry and physical properties of objects as well as occlusions are common challenges for the
        identification of affordances. Occlusions of objects that are hand-held by a person is a particular challenge
        when people are manipulating an object. To address this challenge, we propose an affordance segmentation model
        that uses auxiliary branches to focus on the object and hand regions separately. The proposed model learns
        affordance features under hand-occlusion by weighting the feature map through hand and object segmentation. To
        train the proposed model, we annotated the visual affordances of an existing dataset with mixed-reality images
        of hand-held containers in third-person (exocentric) images. Experiments on both real and mixed-reality show
        that our model achieves better affordance segmentation and generalisation than existing models.</p>
</div>
<!--<h2>Arm-Container Affordance Network (ACANet)</h2>-->
<div class="text">
    <p> We name our proposed model Arm-Container Affordance Network (ACANet) as we consider containers for food and
        drinks, and <em>graspable</em> and <em>contain</em> as affordance classes. Assuming as input the image with at
        the center a
        correctly detected object of interest, the model identifies the classes <em>background</em>, and <em>arm</em>,
        <em>graspable</em> and <em>contain</em>. </p>
    <p> ACANet uses a multi-branch architecture to predict object and hand segmentation, and a fusion module to learn
        separate sets of features in the hand and object region. The additional segmentation branches
        specialise in the segmentation of the arm and of the visible region of the object. Segmenting the object helps
        the model learn the area of the image where the affordances are. To learn specialised features in the object and
        arm regions, we project the affordance features &phi;<sub>a</sub> into two different feature spaces
        maintaining the same dimensionality, &phi;<sub>o</sub> and &phi;<sub>h</sub>, by using C'
        convolutional filters 1x1 to combine each pixel position independently. We then perform a pixel-wise weighting
        of the feature maps &phi;<sub>o</sub> and &phi;<sub>h</sub> with the corresponding segmentation mask
        m<sub>o</sub> and m<sub>h</sub>, i.e., features outside of the predicted object (or arm)
        region are highly penalised. </p>
</div>

<!--<h2>Mixed-reality dataset</h2>
<div class="text">
    <p>Training ACANet requires a large dataset with (exocentric) images of hand-occluded objects and annotated with
        segmentation masks of both arm, object, and affordances. Such a dataset was not available, and collecting and
        manually annotating a new dataset is challenging, expensive, and time-consuming. We complement <a
                href="https://zenodo.org/record/7622793">CORSMAL Hand-Occluded Containers (CHOC)</a>, which has
        mixed-reality images of hand-occluded container for object pose estimation, with visual affordance annotations
        (see Fig. 2).
        Using mixed-reality datasets can easily scale the generation of a larger number of images under different
        realistic backgrounds.</p>
</div>
<figure>
    <img src="./images/choc-aff.png" alt="CHOC-AFF">
    <figcaption>Fig.2: Samples of cropped RGB images and segmentation maps of arms and object affordances from the
        mixed-reality dataset.
        <br/>
        <table>
            <tr>
                <th>Legend:&nbsp;&nbsp;</th>
                <th class="black">&nbsp;&nbsp;</th>
                <th><em>background,&nbsp;&nbsp;</em></th>
                <th class="red">&nbsp;&nbsp;</th>
                <th><em>graspable,&nbsp;&nbsp;</em></th>
                <th class="green">&nbsp;&nbsp;</th>
                <th><em>contain,&nbsp;&nbsp;</em></th>
                <th class="white">&nbsp;&nbsp;</th>
                <th><em>arm</em></th>
            </tr>
        </table>
    </figcaption>
</figure>

<h2>Real data test set</h2>
<div class="text">
    <p>To validate the models in real conditions, we form a benchmarking test set of 300 images, evenly selected from
        two
        existing public datasets for hand-object pose estimation or reconstruction: <a
                href="https://www.tugraz.at/index.php?id=40231">HOnnotate 3D (HO-3D)</a> and <a
                href="https://corsmal.eecs.qmul.ac.uk/containers_manip.html">CORSMAL Containers Manipulation (CCM)</a>
        (see
        Fig.3). This set includes various challenges, such as presence of the human body, real interactions, and
        different
        object instances and hand-object poses.</p>
</div>
<figure>
    <img src="./images/real_data.png" alt="CCM_HO3D">
    <figcaption>Fig.3: Samples of cropped RGB images and segmentation maps of arms and object affordances from the
        real data test set.
        <br/>
        <table>
            <tr>
                <th>Legend:&nbsp;&nbsp;</th>
                <th class="black">&nbsp;&nbsp;</th>
                <th><em>background,&nbsp;&nbsp;</em></th>
                <th class="red">&nbsp;&nbsp;</th>
                <th><em>graspable,&nbsp;&nbsp;</em></th>
                <th class="green">&nbsp;&nbsp;</th>
                <th><em>contain,&nbsp;&nbsp;</em></th>
                <th class="white">&nbsp;&nbsp;</th>
                <th><em>arm</em></th>
            </tr>
        </table>
    </figcaption>
</figure>-->

<h2 class="section">Data for reproducibility</h2>
<div class="text">
    <p> We complement CORSMAL Hand-Occluded Containers (CHOC), which has
        mixed-reality images of hand-occluded containers, with visual affordance annotations.
        Data and annotation are available at <a href="https://doi.org/10.5281/zenodo.5085800">CHOC webpage</a>. </p>
</div>

<h2 class="section">Additional results</h2>
<div class="text">
    <p> We show results arm and affordances results of models on a subset of the real data test set.</p>
</div>
<figure>
    <img src="../images/acanet_qual_res_website.jpg" class="res" alt="HO3D_CCM_res">
    <figcaption><!--Fig.4: Comparison of the predicted (color-coded) affordance and hand masks between the models on sampled
        RGB images from the HO-3D and CCM. The segmentation mask are overlayed on the RGB images.
        <br/>-->
        RN50-F: ResNet50-FastFCN, RN18-U: ResNet18-UNET
        <br/>
        <table>
            <tr>
                <th>Legend:&nbsp;&nbsp;</th>
                <!--<th class="black">&nbsp;&nbsp;</th>
                <th><em>background,&nbsp;&nbsp;</em></th>-->
                <th class="red">&nbsp;&nbsp;</th>
                <th><em>graspable,&nbsp;&nbsp;</em></th>
                <th class="green">&nbsp;&nbsp;</th>
                <th><em>contain,&nbsp;&nbsp;</em></th>
                <th class="blue">&nbsp;&nbsp;</th>
                <th><em>arm</em></th>
            </tr>
        </table>
    </figcaption>
</figure>

<h2 class="section">Reference</h2>
<div class="text">
    If you use the data, the code, or the models, please cite:
    <br>
    <br>
    <strong>Affordance segmentation of hand-occluded containers from exocentric images</strong><br>
    T. Apicella, A. Xompero, E. Ragusa, R. Berta, A. Cavallaro, P. Gastaldo<br>
    IEEE/CVF International Conference on Computer Vision Workshops (ICCVW), 2023
    <br>
    <br>
    <pre> @inproceedings{apicella2023affordance,
  title={Affordance segmentation of hand-occluded containers from exocentric images},
  author={Apicella, Tommaso and Xompero, Alessio and Ragusa, Edoardo and Berta, Riccardo and Cavallaro, Andrea and Gastaldo, Paolo},
  booktitle={IEEE/CVF International Conference on Computer Vision Workshops (ICCVW)},
  year={2023},
}
</pre>
</div>

<h2 class="section">Contact</h2>
<div class="text">
    If you have any further enquiries, question, or comments, please contact <a
        href="mailto:tommaso.apicella@edu.unige.it">tommaso.apicella@edu.unige.it</a>
    <br>
    <br>
</div>

<!--<footer>
    <div class="links">
        <a href="..." class="icon_publication"><i class="fa fa-file-pdf-o" style="font-size:20px;"> Paper</i></a>
        <a href="..." class="icon_publication"><i class="ai ai-arxiv ai-3x" style="font-size:20px;"> arXiv</i></a>
        <a href="..." class="icon_publication"><i class="fa fa-github" style="font-size:20px;"> Code</i></a>
        <a href="..." class="icon_publication"><i class="ai ai-zenodo" style="font-size:20px;"> Models</i></a>
        <a href="..." class="icon_publication"><i class="ai ai-zenodo" style="font-size:20px;"> Dataset</i></a>
    </div>
</footer>-->
</body>
</html>